<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <meta name="description" content="What making lightweight image editing models that run on edge devices entails.">
    <link rel="canonical" href="https://blog.separateconcerns.com/2026-02-07-finegrain.html">
    <link href="css/theme.css?cache=5" rel="stylesheet" type="text/css">
    <link
      rel="alternate" type="application/atom+xml"
      href="https://blog.separateconcerns.com/feed.atom"
    />
    <title>What I do at Finegrain</title>
  </head>
  <body>
    <div id="container">
    <header>
      <h1>What I do at Finegrain</h1>
      <h4>
        published 2026-02-07
        [ <a href="index.html">home</a> ]
      </h4>
    </header>
    <blockquote>
<p>Note: Yes, I use em-dashes when I write, and no, this is not AI. As always I only use &ldquo;AI&rdquo; for spellchecking.</p>
</blockquote>
<p>I have been at <a href="https://finegrain.ai">Finegrain</a> <a href="https://blog.separateconcerns.com/2023-03-31-joining-finegrain.html">for three years</a> so it is time for <a href="https://blog.separateconcerns.com/2021-05-01-inch.html">the next</a> <a href="https://blog.separateconcerns.com/2018-01-06-four-years.html">article in</a> <a href="https://blog.separateconcerns.com/2013-06-20-three-years-proprietary-projects.html">this series</a> where I share a bit about what I’m doing at work.</p>
<section id="What-Finegrain-does">
<h2>What Finegrain does</h2>
<p>I am going to skip <a href="https://finegrain-ai.github.io/refiners/">things we have done in the early days</a> that have become irrelevant and focus on what we are doing now.</p>
<p>So first, we make image editing models. We train them, we evaluate them, and we ship them in a format customers can integrate. We have always focused on keeping the models small and efficient, initially so we could have a fast and cheap API, and now so they can run entirely on edge devices such as mobile phones.</p>
<p>We sometimes build end-user tools to demonstrate those models too. Among those you may have seen in our API era, there was a Web-based image editor, <a href="https://www.comfy.org">ComfyUI</a> nodes, a chatbot for image editing, and an ad generation assistant. All those are gone, but we still have a few public <a href="https://huggingface.co/finegrain/spaces">Hugging Face Spaces</a>. Now our flagship product is <a href="https://finegrain.ai/sdk">the mobile SDK</a>, we have <a href="https://finegrain.ai/app">a free mobile application</a> to demonstrate it.</p>
<p>Now, let&rsquo;s get a bit more into what I am working on exactly.</p>
</section>
<section id="PHX-the-server-side-inference-stack">
<h2>PHX, the server-side inference stack</h2>
<p>The first important thing I have built is our server-side inference stack. What does this include?</p>
<p>A model is a bunch of layers, which basically means matrices of numbers corresponding to operations (multiplications, convolutions, etc) organized into a graph with non-linearities sprinkled in-between. The model takes tensors as input and returns tensors as output.</p>
<p>When you want to do something with a model, you need some preprocessing to feed it data and postprocessing to take it out. In the case of text, that often means tokenizing. In the case of latent image models that means encoding and decoding — which by the way also involve <a href="https://en.wikipedia.org/wiki/Variational_autoencoder">models</a>.</p>
<p>You also need a lot of things to optimize the model&rsquo;s operation. That includes the mode of execution (compilation, quantization, numerical precision, etc). Crucially, in the case of models that you call several times in a loop such as diffusion / flow matching models, this also includes solvers (samplers / schedulers), which can be the most complex and math-heavy part of an inference pipeline.</p>
<p>In our inference stack, which is called PHX (a reference to <a href="https://en.wikipedia.org/wiki/Airbus_A330_MRTT#France">the Phénix airplane</a>), all of this is packaged in a <strong>processor</strong> (which some call a <strong>pipeline</strong>). Processors can be used locally or deployed to <a href="https://modal.com">Modal</a> — which we use e.g. for model evaluation purposes — but in production they are deployed on GPU servers and exposed to callers using <a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/introduction/index.html">NVIDIA&rsquo;s inference server Triton</a>.</p>
</section>
<section id="Disciple-the-Web-backend">
<h2>Disciple, the Web backend</h2>
<p>PHX is not exposed to customers or applications directly. We provide a Web API that is easier to call, asynchronous (because although our inference is fast it is still slower than a typical Web request), and deals with authentication and payment.</p>
<p>In addition to this API, we have a Web backend for customers, a bit of admin and monitoring&hellip; All of this lives in a modular monolith called Disciple, a reference to a character of <a href="https://en.wikipedia.org/wiki/L%C3%A9onard_(comics)">the comic strip Léonard</a> whose most famous line is &ldquo;I serve science, and that is my joy.&rdquo;</p>
<p>The stack is nothing too fancy; I picked technologies I had previous experience with and knew to be reliable. We use <a href="https://quart.palletsprojects.com/">Quart</a> with Blueprints and PostgreSQL. For asynchrony we use <a href="https://nchan.io">Nchan</a> for SSE and <a href="https://beanstalkd.github.io">Beanstalk</a> for background jobs. For efficient image processing we rely on <a href="https://libvips.github.io/pyvips/">VIPS</a>.</p>
</section>
<section id="The-iOS-SDK">
<h2>The iOS SDK</h2>
<p>For some time now, the main focus of Finegrain is running all our models on mobile devices directly. This means we had to port our inference code to run on iOS (for now) using <a href="https://en.wikipedia.org/wiki/Neural_Engine">Apple&rsquo;s Neural Engine (ANE)</a>.</p>
<p>This involves, among other things, compiling and quantizing the model using Apple&rsquo;s <a href="https://apple.github.io/coremltools/docs-guides/">Core ML Tools</a>. This is not a straightforward step; most models cannot be compiled out of the box and require architectural changes first. As for quantization, obtaining high average quantization ratios while retaining precision is a craft that would deserve its own blog post.</p>
<p>Once you have that model, you need the equivalent of PHX&rsquo;s processors, but for the mobile device. I use PHX as the reference codebase, but porting to Swift on iOS is not as straightforward as one may think. On mobile anything can quickly become a bottleneck, and if you are not careful you end up spending more time doing pre- and post-processing than running the model. We use <a href="https://developer.apple.com/documentation/accelerate">Apple&rsquo;s Accelerate framework</a> extensively to speed up those parts.</p>
</section>
<section id="Science-and-hacks">
<h2>Science and hacks</h2>
<p>All of this is advanced but straightforward engineering, but if you want to be good in this field you need to go beyond that and implement cutting-edge research or innovate.</p>
<p>A large part of it is in model training and the datasets we use, of course. I am hardly the person who trains the most models at the company — I do a little, mostly focusing on performance-related things. We all read papers in the domain and discuss approaches to try all the time though. Personally, I have been studying the things &ldquo;around&rdquo; the main model a lot, including solvers, NFE (Number of Function Evaluations) reduction techniques, and recently auto-encoders.</p>
<p>In addition to that, there are a few ideas I have had and implemented that make a difference. The most important one is the set of techniques we use to edit high-resolution images with a model backbone that works at much lower resolution. We call them &ldquo;fixes&rdquo; and let&rsquo;s just say having a background in signal processing and classical computer vision as well as a good understanding of latent spaces helps a lot there. I have spent — and still spend — a lot of time on this, and it is a crucial part of what we do, so if you were to ask me what I am most proud of so far, that would be it.</p>
<p>Another notable thing I have implemented recently is the ability to swap the capabilities (skills) of the CoreML model. Core ML has <a href="https://apple.github.io/coremltools/docs-guides/source/multifunction-models.html">Multifunction Models</a>, but in theory they are built by compiling each function and merging them once on a Mac. With the tooling I have designed, we can create new functions for a given backbone in pure PyTorch, and add / remove / swap functions in the model almost instantly on any machine including an iPhone. This makes it possible, for instance, to update a single function in an application without re-downloading the whole model.</p>
<p>As always, I haven&rsquo;t talked about <strong>everything</strong> I do. We are a small company and I am still a jack of all trades kind of guy, so I always write tooling and fix bugs here and there. That should give you a decent idea of what my day-to-day is about. As always, if something piqued your interest, feel free to <a href="https://catwell.info">get in touch</a>.</p>
</section>

    </div>
    <div id="footer">
      [ <a href="index.html">home</a> ]
    </div>
    
    <script
      data-goatcounter="https://separateconcerns.goatcounter.com/count"
      async src="//gc.zgo.at/count.js"></script>
  </body>
</html>
